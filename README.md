# Generative UI with Flask and Ollama

This project is a web-based application that demonstrates how to build a real-time, AI-driven user interface using a Python Flask backend and a locally running Ollama language model. Users can type natural language commands to dynamically add, update, or remove UI elements. Furthermore, the application listens to user interactions with these elements (like button clicks), sending the context of the interaction back to the language model to generate further UI modifications. This creates a fully interactive experience where the LLM acts as both the UI generator and the event-handling logic.

## Features

* **Real-Time UI Updates**: Leverages Server-Sent Events (SSE) to push UI changes from the server to the client without needing to refresh the page.
* **Local LLM Integration**: Connects to a locally running Ollama instance, ensuring privacy and no API costs.
* **Dynamic Model Selection**: A dropdown menu allows you to select any of your locally available Ollama models on the fly.
* **Live State Preview**: Displays the current JSON representation of the UI for easy debugging and understanding.
* **Simple and Clean Interface**: Built with Tailwind CSS for a modern and responsive user experience.

## Tech Stack

* **Backend**: Python 3, Flask
* **LLM Engine**: Ollama
* **Frontend**: HTML, JavaScript, Tailwind CSS
* **Real-Time Communication**: Server-Sent Events (SSE)

## Setup and Installation

Follow these steps to get the project running on your local machine. This guide assumes you are on a Linux-based system like Ubuntu.

### Step 1: Prerequisites

* **Python 3.11+**: Ensure you have a compatible version of Python installed. You can manage Python versions with `pyenv`.
* **Ollama**: You must have Ollama installed and running. You can download it from [ollama.com](https://ollama.com).

### Step 2: Clone the Repository

```bash
git clone https://github.com/ruapotato/generative-ui
cd generative-ui
```

### Step 3: Set Up a Virtual Environment

It's highly recommended to use a virtual environment to manage project dependencies.

```bash
# Create a virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate
```

### Step 4: Install Dependencies
pip install flask requests

### Step 5: Prepare Ollama

1. **Start the Ollama service**. This usually runs in the background after installation. You can check its status with your system's service manager or by running `ollama serve`.
2. **Pull a model**. You need at least one model available for the application to use.

```bash
# Pull the mistral model (or any other model you prefer)
ollama pull mistral

# Pull another model to test the dropdown
ollama pull llama3
```

## Running the Application

1. Make sure your Python virtual environment is activated.
2. Ensure the Ollama service is running in the background.
3. Start the Flask development server from the project's root directory.

```bash
python run.py
```

4. Open your web browser and navigate to `http://127.0.0.1:5000`.

## Usage

1. The application will load and populate the "Select Model" dropdown with your available Ollama models.
2. Choose the model you wish to use.
3. In the input box, type a command describing a UI change (e.g., "add a button named submit", "change the greeting to Hello David", "add a dropdown with options apple, banana, cherry").
4. Click "Generate" or press Enter.
5. The UI on the left will update in real-time based on the JSON command generated by the language model.

## License

This project is licensed under the GNU General Public License v3.0. See the `LICENSE` file for full details.

Copyright (C) 2025 David Hamner
